---
title: 'P&S-2021: Lab assignment 2'
author: "Olha Liuba, Mykhailo-Markiian Tsalyk, Ostap Dutka"
output:
  html_document:
    df_print: paged
---
## Responsibilities
Task 1: Olha Liuba  
Task 2: Mykhailo-Markiian Tsalyk  
Task 3: Ostap Dutka  

## 	General comments and instructions
*  Complete solution will give you $\bf 3$ points (out of 100 total). Submission deadline is **09:00 of 08 November 2021**  
*  The report must be prepared as an _R notebook_; you must submit to **cms** both the source _R notebook_ **and** the generated html file  
*  At the beginning of the notebook, provide a work-breakdown structure estimating efforts of each team member  
*  For each task, include 
    +  problem formulation and discussion (what is a reasonable answer to discuss);  
    +  the  corresponding $\mathbf{R}$ code with comments (usually it is just a couple of lines long);  
    +  the statistics obtained (like sample mean or anything else you use to complete the task) as well as histograms etc to illustrate your findings;  
    +  justification of your solution (e.g. refer to the corresponding theorems from probability theory);  
    +  conclusions (e.g. how reliable your answer is, does it agree with common sense expectations etc)  
*  The __team id number__ referred to in tasks is the __two-digit__ ordinal number of your team on the list. Include the line __set.seed(team id number)__ at the beginning of your code to make your calculations reproducible. Also observe that the answers **do** depend on this number!  
*  Take into account that not complying with these instructions may result in point deduction regardless of whether or not your implementation is correct. 


### Task 1

#### In this task, we discuss the \([7,4]\) Hamming code and investigate its reliability. That coding system	can correct single errors in the transmission of \(4\)-bit messages and proceeds as follows:   

* given a message \(\mathbf{m} = (a_1 a_2 a_3 a_4)\), we first encode it to a \(7\)-bit _codeword_ \(\mathbf{c} = \mathbf{m}G = (x_1 x_2 x_3 x_4 x_5 x_6 x_7)\), where \(G\) is a \(4\times 7\) _generator_ matrix  
* the codeword \(\mathbf{c}\) is transmitted, and \(\mathbf{r}\) is the received message  
* \(\mathbf{r}\) is checked for errors by calculating the _syndrome vector_ \(\mathbf{z} := \mathbf{r} H\), for a \(7 \times 3\) _parity-check_ matrix \(H\)  
* if a single error has occurred in \(\mathbf{r}\), then the binary \(\mathbf{z}  = (z_1 z_2 z_3)\) identifies the wrong bit no. \(z_1 + 2 z_2 + 4z_3\); thus \( (0 0 0)\) shows there was no error (or more than one), while \((1 1 0 )\) means the third bit (or more than one) got corrupted  
* if the error was identified, then we flip the corresponding bit in \(\mathbf{r}\) to get the corrected \(\mathbf{r}^* = (r_1 r_2 r_3 r_4 r_5 r_6 r_7)\);  
* the decoded message is then \(\mathbf{m}^*:= (r_3r_5r_6r_7)\). 
  
#### The __generator__ matrix \(G\) and the __parity-check__ matrix \(H\) are given by
\[	
	G := 
	\begin{pmatrix}
		1 & 1 & 1 & 0 & 0 & 0 & 0 \\
		1 & 0 & 0 & 1 & 1 & 0 & 0 \\
		0 & 1 & 0 & 1 & 0 & 1 & 0 \\
		1 & 1 & 0 & 1 & 0 & 0 & 1 \\
	\end{pmatrix},
 \qquad 
	H^\top := \begin{pmatrix}
		1 & 0 & 1 & 0 & 1 & 0 & 1 \\
		0 & 1 & 1 & 0 & 0 & 1 & 1 \\
		0 & 0 & 0 & 1 & 1 & 1 & 1
	\end{pmatrix}
\]


#### Assume that each bit in the transmission \(\mathbf{c} \mapsto \mathbf{r}\) gets corrupted independently of the others with probability \(p = \mathtt{id}/100\), where \(\mathtt{id}\) is your team number. Your task is the following one.

1.  Simulate the encoding-transmission-decoding process \(N\) times and find the estimate \(\hat p\) of the probability \(p^*\) of correct transmission of a single message \(\mathbf{m}\). Comment why, for large \(N\), \(\hat p\) is expected to be close to \(p^*\).  
2. By estimating the standard deviation of the corresponding indicator of success by the standard error of your sample and using the CLT, predict the \emph{confidence} interval \((p^*-\varepsilon, p^* + \varepsilon)\), in which the estimate  \(\hat p\) falls with probability at least \(0.95\).  
3.  What choice of \(N\) guarantees that \(\varepsilon \le 0.03\)?  
4.  Draw the histogram of the number \(k = 0,1,2,3,4\) of errors while transmitting a \(4\)-digit binary message. Do you think it is one of the known distributions?


#### You can (but do not have to) use the chunks we prepared for you 

#### First, we set the **id** of the team and define the probability \(p\) and the generator and parity-check matrices \(G\) and \(H\)

```{r}
# your team id number 
                          ###
id <- 12                  ### Change to the correct id!
                          ###
set.seed(id)
p <- id/100
experiments = 100
# matrices G and H
G <- matrix(c(1, 1, 1, 0, 0, 0, 0,
		1, 0, 0, 1, 1, 0, 0,
		0, 1, 0, 1, 0, 1, 0,
		1, 1, 0, 1, 0, 0, 1), nrow = 4, byrow = TRUE)
H <- t(matrix(c(1, 0, 1, 0, 1, 0, 1,
		0, 1, 1, 0, 0, 1, 1,
		0, 0, 0, 1, 1, 1, 1), nrow = 3, byrow = TRUE))
#cat("The matrix G is: \n") 
#G  
#cat("The matrix H is: \n") 
#H
#cat("The product GH must be zero: \n")
#(G%*%H) %%2
```

#### Next, generate the messages

```{r}
# generate N messages
# c(0,1) is a vector -- we have a choice of 0 or 1
# with sample we choose either 0 or 1 4 times for each message, n messages
# replace = TRUE allows to have repeated 0's and 1's, otherwise we would have a matrix of only size 2 
message_generator <- function(N) {
  matrix(sample(c(0,1), 4*N, replace = TRUE), nrow = N)
}  
messages <- message_generator(experiments)
# multiply messages matrix with G matrix and take mod 2 (because there may be values greater than 1)
codewords <- (messages %*% G) %% 2
```
#### Generate random errors; do not forget that they occur with probability \(p\)! Next, generate the received messages

```{r}
# errors is the matrix with 0's and 1's, where
# 0 means that no error occurred in this bit;
# 1 means that there was an error
error_generator <- function(N) {
  matrix(sample(c(0,1), 7*N, replace = TRUE, prob = c((1 - p), p)), nrow = N)
  # probability of no error is 1 - p; probability of an error is p
}
errors <- error_generator(experiments)
# generate error matrix (the size is the same as of the messages matrix)
# make xor of messages and errors -> if there is an error in a bit (value 1), its value from messages matrix changes in the received matrix
# otherwise the bit remains the same
# multiply matrix to 1 to change boolean values to 0's and 1's
received <- xor(codewords, errors) * 1
```

The next steps include detecting the errors in the received messages, correcting them, and then decoding the obtained messages. After this, you can continue with calculating all the quantities of interest

### Detect errors in the messages, correct and decode them

```{r}
# syndrome vector identifies the bits where the error may have occurred
# if 000 - no error, if other - error at the position that is identified by column of H
syndrome_v <- (received %*% H) %% 2

# find the positions from H matrix of errors
bit_errors <- apply(syndrome_v, 1, function(x) x[1] * 1 + x[2] * 2 + x[3] * 4)

# correct messages by changing the possibly wrong bits at the positions
corrected <- received
for (i in 1:experiments) {
  corrected[i, bit_errors[i]] <- 1 - received[i, bit_errors[i]]
}

# get decoded messages from corrected by excluding parity check bits: take only columns that are not the power of 2
decoded <- corrected[, c(3,5,6,7)]
```

### Count wrongly decoded messages and find the probability p^ - the estimating probability of correct transmission of a single message

```{r}
# create vector with number of errors in each message -- we will need it in to build histogram in part 3
errors <- vector(mode = "integer", length = experiments)
for (i in 1:experiments) {
  for (j in 1:4) {
    errors[i] <- errors[i] + xor(messages[i, j], decoded[i, j])
  }
}
# get vector with indicators of correct transmission of each message
correct_indicators <- !as.logical(errors) * 1
# count all non-zero values in the vector to find amount of correctly-decoded messages
num_wrong_m <- sum(correct_indicators)

p_hat <- num_wrong_m/experiments
print(p_hat)
```

### Estimate standard deviation of the corresponding indicator of success by the standard error, find half-length of the confidence interval, which contains which contains the true value p∗ with probability at least 0.95 and minimum n, at which half-length is <= 0.03

```{r}
st_dev <- sd(correct_indicators)
# We want to find boundary value that determines the probability of 0.95
# P(X < a) = param
# P(X < a) - P(X < -a) = 0.95 - desirable probability
# P(X < -a) = P(X > a) = (1 - (P(X < a) - P(X < -a))) / 2 = (1 - 0.95) / 2 = 0.05 / 2 = 0.025
# P(X < -a) = P(X > a) because the curve is symmetric
# P (X < a) = 0.95 + P(X < -a) = 0.95 + 0.025 = 0.975
# To find a, we can use function qnorm(area) that will find the boundary value for that area
between_area <- 0.95
left_nonsuitable <- (1 - between_area) / 2
left_area <- between_area + left_nonsuitable
boundary <- qnorm(left_area)

half_length <- boundary * st_dev / sqrt(experiments)
print(half_length)

# second part of task - find minimum experiments to obtain half_length = 0.03
given_half_length <- 0.03
min_experiments = ceiling((boundary * st_dev / given_half_length) ** 2)
print(min_experiments)
```
### Draw the histogram of the number k = 0, 1, 2, 3, 4 of errors while transmitting a 4-digit binary message

```{r}
error_nums <- vector(mode = "integer", length = 5)
for (i in 1:5) {
  error_nums[i] = sum(errors == (i-1))
}
histogramm <- hist(errors,
main="Number of errors while transmitting a 4-digit binary code",
xlab="Number of errors in a single message",
col="dark green",
xlim=c(-1,4),
breaks = -1:4)
```

### Task 1. Conclusions

1) We found \(\hat p\) - the estimation of the probability \(p^*\) of correct transmission of a single message. For 100 messages it is 0.81. According to Strong Law of Large Numbers, \(\hat p\) is expected to be close to \(p^*\) for large number of experiments, as P(|\(\hat p\) - \(p^*\)| >= \(\varepsilon\)) \(\rightarrow 0\) as n \(\rightarrow \infty\).

2) By estimating the standard deviation of the corresponding indicator of success by the standard error of our sample, we found the half-length \(\varepsilon\) of the confidence interval (\(\hat p\) − \(\varepsilon\), \(\hat p\) + \(\varepsilon\)ε), which contains the true value \(p^*\) with probability at least 0.95. Here we used CLT and obtained result that for 100 messages \(\varepsilon\) is 0.07727692, and with the standard deviation as the error of sample of 100 messages we found that there has to be at least 664 messages to obtain \(\varepsilon <= 3\). This value changes with different samples.

3) We drew histogram of the number k = 0, 1, 2, 3, 4 of errors while transmitting a 4-digit binary message. This is none of the known distribution, because we cannot say anything about the number of errors that occurred. We know that the number of errors is definitely not a continuous random variable, so the only option would be a discrete r. v. However, they all depend on the number of Bernoulli trials, which is not our case. We only have the probability of correct transmission of a single message but we cannot obtain the number of errors in a message and state that it is one of the known distributions. Known distribution would be if for different number of experiments (n) we counted the number of errors / correctly decoded messages. That would be binomial distribution. In our case we don't have Bernoulli trials, and so we also don't have any known distribution.

### Task 2. 
#### 		In this task, we discuss a real-life process that is well modelled by a Poisson distribution. As you remember, a Poisson random variable describes occurrences of rare events, i.e., counts the number of successes in a large number of independent random experiments. One of the typical examples is the __radioactive decay__ process.
		
#### Consider a sample of radioactive element of mass $m$, which has a big _half-life period_ \(T\); it is vitally important to know the probability that during a one second period, the number of nuclei decays will not exceed some critical level \(k\). This probability can easily be estimated using the fact that, given the _activity_ ${\lambda}$ of the element (i.e., the probability that exactly one nucleus decays in one second) and the number $N$ of atoms in the sample, the random number of decays within a second is well modelled by Poisson distribution with parameter $\mu:=N\lambda$. Next, for the sample of mass $m$, the number of atoms is $N = \frac{m}{M} N_A$, where $N_A = 6 \times 10^{23}$ is the Avogadro constant, and $M$ is the molar (atomic) mass of the element. The activity of the element, $\lambda$, is $\log(2)/T$, where $T$ is measured in seconds. 

#### Assume that a medical laboratory receives $n$ samples of radioactive element ${{}^{137}}\mathtt{Cs}$ (used in radiotherapy) with half-life period $T = 30.1$ years and mass \(m = \mathtt{team\, id \,number} \times 10^{-6}\) g each. Denote by $X_1,X_2,\dots,X_n$ the __i.i.d.  r.v.__'s counting the number of decays in sample $i$ in one second. 

1.  Specify the parameter of the Poisson distribution of \(X_i\) (you'll need the atomic mass of _Cesium-137_)  
2.  Show that the distribution of the sample means of \(X_1,\dots,X_n\) gets very close to a normal one as $n$ becomes large and identify that normal distribution. To this end,
    +  simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;
    +  repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and form the empirical cumulative distribution function \(\hat  F_{\mathbf{s}}\) of $\mathbf{s}$;
    +  identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} \(F\) of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} \(\hat F_{\mathbf{s}}\) and plot both __c.d.f.__'s on one graph to visualize their proximity (use the proper scales!);
    +  calculate the maximal difference between the two \textbf{c.d.f.}'s;
    +  consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.   
3.  Calculate the largest possible value of $n$, for which the total number of decays in one second is less than $8 \times 10^8$ with probability at least $0.95$. To this end,  
    +  obtain the theoretical bound on \(n\) using Markov inequality, Chernoff bound and Central Limit Theorem, and compare the results;  
    +  simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sum $s=x_1 + \cdots +x_n$;
    +  repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of sums;
    +  calculate the number of elements of the sample which are less than critical value ($8 \times 10^8$) and calculate the empirical probability; comment whether it is close to the desired level \(0.95\)

```{r}
set.seed(12)
lambda <- log(2) / (30.1 * 365 * 24 * 3600)
N <- 12 * 6  * 10^17 / 139
mu <- N * lambda
K <- 1e3
n <- 20
sample_means <- colMeans(matrix(rpois(n*K, lambda = mu), nrow=n))
sample_summ <- sample_means * n
```


#### Next, calculate the parameters of the standard normal approximation

```{r}
mu <- mu
sigma <- sqrt(mu/n)
```

#### We can now plot ecdf and cdf

```{r}
xlims <- c(mu - 3*sigma, mu + 3*sigma)
Fs <- ecdf(sample_means)
plot(Fs, 
     xlim = xlims, 
     ylim = c(0,1),
     col = "blue",
     xlab = "decays of molecul/sec",
     lwd = 2,
     main = "Comparison of ecdf and cdf")
curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)
```

### Let's calculate maximal difference between c.d.f.'s
```{r}
t <- seq(min(sample_means), max(sample_means), by=1)
max(abs(Fs(t) - pnorm(t, mu, sigma)))
```

### Next, consider cases, where \(n = 5; 10; 50\)
```{r}
draw <- function() {
  for (n in c(5, 10, 50)){
    set.seed(12)
    sample_means <- colMeans(matrix(rpois(n*K, lambda = mu), nrow=n))
    sigma <- sqrt(mu/n)
    
    xlims <- c(mu - 3*sigma, mu + 3*sigma)
    Fs <- ecdf(sample_means)
    plot(Fs, 
         xlim = xlims, 
         ylim = c(0,1),
         col = "blue",
         xlab = "decays of molecul/sec",
         lwd = 2,
         main = paste("Comparison of ecdf and cdf while n =", n))
    curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)
  }
}
```

### Here our plots
```{r}
draw()
```

##### We got such results, because theoretically, Central Limit Theorim gives good approximation of normal distribution for most distributions with \(n >= 30\)

### Find amount of sums which are less than \(8 * 10^8\) and emperical probability
```{r}
n <- 5
counter <- 0
sample_means <- colMeans(matrix(rpois(n*K, lambda = mu), nrow=n))
sample_summ <- sample_means * n

for (summ in sample_summ) {
  if (summ < 8 * 10^8) {
    counter = counter + 1
  }
}

counter
counter / length(sample_means)
```


### Now let's calculate \(n_\max\) such that \(S_n < 8 * 10^8\) with \(probability \ge 0.95\)
```{r}
success <- 0
n <- 1

while (success / length(sample_summ) >= 0.95 | n == 1) {
  sample_means <- colMeans(matrix(rpois(n*K, lambda = mu), nrow=n))
  sample_summ <- sample_means * n
  success = 0
  
  for (summ in sample_summ) {
    if (summ < 8 * 10^8) {
      success = success + 1
    }
  }
  n = n + 1
}

n = n - 2
n
```


### Theoretical bound

\(E[S_n] = n*\mu\)  
\(Var[Sn] = n*\mu\)  
\(\sigma = \sqrt{n*\mu}\)  
\(P(X \le a)\)

#### Markov inequality
\(1-P(X \ge a) < \frac{\mu}a\)  
\(P(X \ge a) > 1 - \frac{\mu}a\)

#### Chernoff bound
\(P(X \ge a) \le e^{-\phi(a)}\)  
\(\phi(a) = \max\{{s*a - log{M(s)}}\}\)  
\(M(s) = e^{n*\mu*(e^{s}-1)}\)  
Then, if we tried to calculate a derivative and maximize \(\phi(a)\), it would be $-\infty$. It means that Chernoff bound is useless in our case, because for all $a$ we get that \(P(X \ge a) \le \infty\).

#### Central Limit Theorem
\(P(\frac{S_n - n*\mu)}{\sigma * \sqrt{n}}) \le a) = Ф(a)\)  
So from here, we need to solve the following equation:  
\(\Phi(\frac{8*10^8 - n*\mu}{\sigma*\sqrt{n}}) \ge 0.95\)  
We can do this by finding $x$ while \(\Phi(x) = 0.95\)  
Let's do this  

```{r}
x <- qnorm(0.95)
x
```

We got that \(x \approx 1.6\)  
Then, solving equation, we got that our \(n = 21\)  

```{r}
# P(Sn <= 8*10^8)
n <- 21
# Markov
max(1 - mu*n/(8*10^8), 0)

# CLT
pnorm((8*10^8 - n*mu) / (sqrt(mu*n)))

# Chernoff
exp(-8*10^8) * exp(mu*n*(exp(1)-1))
```


To conclude:

1) First of all we defined parameter of the Poisson distribution which is 37 824 226.We obtained this by multiplying number of existing molecules by probability of the molecule being decayed. Then we got average number of molecules which decay in a second.

2) In the second part we simulated random variables and calculated sample means for each experiment. Then we compared cdf of normal distribution with ecdf of our sample means. Were found parameters for normal distribution which is the closest for our sample data. We found them using Central Limit Theorem and its property that our distribution is approximately normal for large number of random variables with parameters $\mu$ = $\mu_0$ and $\sigma$ = $\frac{\sigma_0}{\sqrt{n}}$. Finally, we can confirm, that as $n$ gets larger our i.i.d.r.v's converges to normal distribution with defined $\mu$ and $\sigma^2$. Moreover, while $n \ge 30$ it is almost normal itself.

3) In the last part, we K times simulated $x_{1}, x_{2}, \dots, x_{n}$ and found sum 	$s_k = x_{1_k} + x_{2_k} + \cdots + x_{n_k}$. Then we calculated $n_{max}$ such that $s_n \le 8*10^8 $ with probability $p \ge 0.95$. Using Markov inequality we checked this result and found a bound for our $P(S_n \le 8* 10^8)$. Using Central Limit Theorem we came to the same result as in calculating empirical probability, that $n_{max} = 21$. We saw that Chernoff bound is useless in our case, because it told us that our probability is less than infinity which is obvious.



### Task 3. 
#### In this task, we use the Central Limit Theorem approximation for continuous random variables. 
#### One of the devices to measure radioactivity level at a given location is the Geiger counter. When the radioactive level is almost constant, the time between two consecutive clicks of the Geiger counter is an exponentially distributed random variable with parameter $\nu_1 = \mathtt{team\,id\,number} + 10$. Denote by \(X_k\) the random time between the \((k-1)^{\mathrm{st}}\) and \(k^{\mathrm{th}}\) click of the counter. 

1.  Show that the distribution of the sample means of \(X_1, X_2,\dots,X_n\) gets very close to a normal one (which one?) as $n$ becomes large.  To this end,
    +  simulate the realizations $x_1,x_2,\dots,x_n$ of the \textbf{r.v.} $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;  
    +  repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and then the \emph{empirical cumulative distribution} function \(F_{\mathbf{s}}\) of $\mathbf{s}$;  
    +  identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} \(F_{\mathbf{s}}\) of and plot both \textbf{c.d.f.}'s on one graph to visualize their proximity;  
    +  calculate the maximal difference between the two \textbf{c.d.f.}'s;  
    +  consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results. 		
    	
2.  The place can be considered safe when the number of clicks in one minute does not exceed $100$. It is known that the parameter $\nu$ is inversely proportional to the number $N$ of the radioactive samples, i.e., \(\nu = \nu_1/N\), where \(\nu_1\) is the parameter for one sample. Determine the maximal number of radioactive samples that can be stored in that place so that, with probability \(0.95\), the place is identified as safe. To do this,  
    +  express the event of interest in terms of the \textbf{r.v.} $S:= X_1 + \cdots + X_{100}$;  
    +  obtain the theoretical bounds on \(N\) using the Markov inequality, Chernoff bound and Central Limit Theorem and compare the results;  
    +  with the predicted \(N\) and thus \(\nu\), simulate the realization $x_1,x_2,\dots,x_{100}$ of the $X_i$ and of the sum \(S = X_1 + \cdots + X_{100}\);  
    +  repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of total times until the \(100^{\mathrm{th}}\) click;  
    +  estimate the probability that the location is identified as safe and compare to the desired level \(0.95\)

#### First, generate samples an sample means: 

```{r}
set.seed(12)
nu1 <- 1/22  # change this! changed to id num + 10
K <- 1e3   # number of experiments
n <- 5     # sample size
# n - number of observations
# rate - rate of vectors = lambda
sample_means <- colMeans(matrix(rexp(n*K, rate = 1/nu1), nrow=n))

# if we increase our sample size n, the mean of sample_means would be closer to the actual mean of exponential distrinution
mean(sample_means)
```

#### Next, calculate the parameters of the standard normal approximation
```{r}
mu <- nu1
sigma <- sqrt(nu1**2/n)
```

#### We can now plot ecdf and cdf
if we increase n, the maximum difference between two cdf's would decrease, and both cdf's would be more similar.

```{r}
xlims <- c(mu-3*sigma,mu+3*sigma)
Fs <- ecdf(sample_means)
plot(Fs, 
     xlim = xlims,
     col = "blue",
     lwd = 2,
     main = "Comparison of ecdf and cdf")
curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)

# calculate max diff between two cdf's
t <- seq(min(sample_means), max(sample_means), by=1)
diff <- max(abs(Fs(t) - pnorm(t, mu, sigma)))
diff
```

## Second part
#### express the event of interest in terms of the r.v. S := X1 + · · · + X100;
we can express the event S as **S >= 1**, its time we have 100 clicks on the Geiger counter and that time must be equal or greater than 1 minute.<br>

*our parameter lambda here is different, in task it's specified as: nu = nu1 x N*

**Lets find N value using Markov inequality:**<br>
P(S >= 1) >= 0.95 - if we time of obtaining 100 clicks is greater than 1min, then with probability 0.95 place is safe<br>
P(S < 1) <= 0.05  - if we have more clicks per minute, than 100. Lets find an upper bound for N (number of radioactive samples)<br>

P(S<1) = P(2-S>1) <= 1/1\*E(2-S) = 2-E(S) = 2-100/(nu1 \* N)<br>
2 - 100/(nu1 * N) <= 0.05<br>
N <= 100/1.95*nu1<br>
N <= 2.3

**and Chebyshev inequality:**<br>
P(S<1) = P(-S >= -1) = P(100/nu1\*N - S >= 100/(nu1\*N) - 1) <= 100/(100-nu1*N)^2<br>
100/(100-nu1*N)^2 <= 0.05<br>
N <= 2.5

**N max = 2**

Here if we would increase N value (number of radioactive parts) then the probability that the place is safe would decrease.
If N=2, probability that place is safe = 1
```{r}
# this value was obtained after applying Markov and Chebyshev inequalities
N = 2

# realization of x1, x2, ..., x100
# rexp(100, rate = (1/nu1) * N) # <- each value represents time in minutes between two ticks
# matrix(rexp(100*K, rate = 22 * N), nrow=100) <- here we get matrix with K cols and 100 rows. K - number of experiments

# here we get mean values of each of K experiments, until 100th klick.
sample_means2 = colMeans(matrix(rexp(100*K, rate = (1/nu1) * N), nrow=100))

# estimate the probability that the location is identified as safe
safe = 0
for (mean_time in sample_means2){
  if (mean_time * 60 >= 0.6){
    safe = safe + 1
  }
}
safe / 1000
```
